# -*- coding: utf-8 -*-
"""Grou5 HW3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/146Ittag7GAThYD5eUd_y1blJR1P9uQpK

# Writeup

For the writeup, please include the following information:


*   Describe the architecture of your neural network
*   Discuss how you tuned your network and why you think it's performance is reasonable for this task
*   Discuss wether or not you feel that this classifier is appropriate for the given task (check PA3 description)

We are using the tenseorflow and pytorch to create a concolutional Neural Network. It has four layers in total, including one input layer, 2 hidden layer and one output layer.  The epoch we set is 10, in order to get higher accuracy.

The input layer shape is 28*28*1, inside the hidden layer, we put two convolution layers(relu), and two pooling layers. After the hidden layer, we flattern those hidden layers and fully connected to the output layer. The final accuracy we get is 98%, which is pretty high and the loss we get is 0.2028.

Our classifier is appropriate for the given tast because first, our input layer is 28*28, which fits the grayscale image size, then out accuracy is greater than 91%, and last, out model is pretty complex, so it may take longer time than regular cnn model, but the accuracy will be higher. 



On the other hand, we also created a Deep Neural Network using PyTorch in order to complete the challenge. In the PyTorch segment, we initially generated 2 Convolutional filters and 2 maxpooling, as well as a Dropout layer with a value of 30%, followed by three dense layers containing 1024 neurons, 256 neurons, 32 neurons, and 10 output neurons (which include class 0-9 for the identification of clothes). In addition, we employed 25 epochs and a 0.01 learning rate to achieve 90% accuracy. After 25 epochs, our computer generated the following result: "Test set: Average loss: 0.2972; Accuracy: 8976/10000 (90%)."

# PyTorch

## 0 Imports
"""

import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import matplotlib.pyplot as plt

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

"""## 1. Set Hyperparameters"""

# You can modify these values

n_epochs = 25
batch_size_train = 64
batch_size_test = 1000
learning_rate = 0.01
momentum = 0.5
log_interval = 100

"""## 2. Load Data"""

train_set = torchvision.datasets.FashionMNIST(
  root = './data/FashionMNIST',
  train = True,
  download = True,
  transform = transforms.Compose([
      transforms.ToTensor()                                 
  ]))

train_loader = torch.utils.data.DataLoader(train_set,
                                     batch_size = batch_size_train,
                                     shuffle=True)

train_set = torchvision.datasets.FashionMNIST(
  root = './data/FashionMNIST',
  train = False,
  download = True,
  transform = transforms.Compose([
      transforms.ToTensor()                                 
  ]))

test_loader = torch.utils.data.DataLoader(train_set,
                                     batch_size = batch_size_train,
                                     shuffle=True)

examples = enumerate(test_loader)
batch_idx, (example_data, example_targets) = next(examples)

fig = plt.figure()
for i in range(6):
  plt.subplot(2,3,i+1)
  plt.tight_layout()
  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')
  plt.title("Ground Truth: {}".format(example_targets[i]))
  plt.xticks([])
  plt.yticks([])

INPUT_CHANNELS = example_data.shape[1]
OUTPUT_CLASSES = 10
example_data.shape

"""## 3. Define Model"""

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # Your code goes here
        self.conv1 = nn.Conv2d(INPUT_CHANNELS, 32, 5, 1)
        self.conv2 = nn.Conv2d(32, 64, 5, 1)
        # self.conv3 = nn.Conv2d(64, 128, 5, 1)
        # put 32 and 64(64x1024 and 320x256)
        # put 28 and 56 (64x896 and 320x256)
        # self.conv3 = nn.Conv2d(20, 40, 5, 1)
        self.maxPooling = nn.MaxPool2d(2)
        self.dropOut = nn.Dropout(p=0.3)
        self.fc1 = nn.Linear(1024, 256)
        # self.fc2 = nn.Linear(512, 256)
        # self.fc3 = nn.Linear(256, 128)
        # self.fc4 = nn.Linear(128, 64)
        self.fc2 = nn.Linear(256, 32)
        self.fc3 = nn.Linear(32, OUTPUT_CLASSES)

    def forward(self, x):
        # And here
        x = self.conv1(x)
        x = self.maxPooling(x)
        x = self.conv2(x)
        x = self.maxPooling(x)
        # x = self.conv3(x)
        # x = self.maxPooling(x)
        x = self.dropOut(x)
        linearize = nn.Flatten()
        x = linearize(x)
        x = self.fc1(x)
        # x = self.fc2(x)
        # 
        # x = self.fc4(x)
        x = self.fc2(x)
        x = self.fc3(x)
        output = F.log_softmax(x, dim=1)
        return output

net = Net().to(device)
net

"""## 4. Train Model"""

train_losses = []
train_counter = []
test_losses = []
test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]

optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)
criterion = nn.MSELoss()

def train(model, train_loader, optimizer, epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data = data.to(device)
        target = target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        if batch_idx % log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.item()))


def test(model, test_loader):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data = data.to(device)
            target = target.to(device)
            output = model(data)
            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss
            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability
            correct += pred.eq(target.view_as(pred)).sum().item()

    test_loss /= len(test_loader.dataset)

    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))

for epoch in range(1, n_epochs + 1):
  train(net, train_loader, optimizer, epoch)
  test(net, test_loader)

"""# Tensorflow

## 0. Imports
"""

import tensorflow as tf
import tensorflow_datasets as tfds

"""## 1. Set Hyperparameters"""

# Tune these

n_epochs = 20
batch_size_train = 64
batch_size_test = 1000
learning_rate = 0.01

"""## 2. Import Data"""

(ds_train, ds_test), ds_info = tfds.load(
    'fashion_mnist',
    split=['train', 'test'],
    shuffle_files=True,
    as_supervised=True,
    with_info=True,
)

def normalize_img(image, label):
  """Normalizes images: `uint8` -> `float32`."""
  return tf.cast(image, tf.float32) / 255., label

ds_train = ds_train.map(
    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
ds_train = ds_train.cache()
ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)
ds_train = ds_train.batch(batch_size_train)
ds_train = ds_train.prefetch(tf.data.AUTOTUNE)

ds_test = ds_test.map(
    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
ds_test = ds_test.batch(batch_size_test)
ds_test = ds_test.cache()
ds_test = ds_test.prefetch(tf.data.AUTOTUNE)

"""## 3. Define Model"""

from tensorflow import keras
from sklearn.metrics import confusion_matrix
import itertools
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from tensorflow.keras.optimizers import RMSprop,Adam
from keras.callbacks import ReduceLROnPlateau
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Flatten

img_row = 28
img_col = 28
batch_size = 128

img_shape = (img_row, img_col, 1)

model = Sequential([
    Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=img_shape),
    MaxPooling2D(pool_size=2),
    Dropout(0.2),
    Flatten(),
    Dense(32, activation='relu'),
    Dense(10, activation='softmax')
])

loss = 'sparse_categorical_crossentropy'
optim = Adam(learning_rate=0.001)
metrics = ['accuracy']

model.compile(loss=loss, optimizer=optim, metrics=metrics)
model.summary()

"""## 4. Train Model"""

# model.compile(loss=loss, optimizer=optim, metrics=metrics)

history= model.fit(
    ds_train,
    epochs=n_epochs,
    validation_data=ds_test,
)

plt.plot(history.history['loss'], color='b', label="validation loss")
plt.title("Test Loss")
plt.xlabel("Number of Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

plt.plot(history.history['val_accuracy'], color='b', label="validation accuracy")
plt.title("Test accuracy")
plt.xlabel("Number of Epochs")
plt.ylabel("accuracy")
plt.legend()
plt.show()